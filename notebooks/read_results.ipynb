{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'wow'  # Change this: options = [cnn_dailymail, wow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_path = '../eval_results'\n",
    "\n",
    "files = os.listdir(eval_results_path)\n",
    "files = [os.path.join(eval_results_path, fname) for fname in files]\n",
    "\n",
    "results = {}\n",
    "for fname in files:\n",
    "    if not fname.endswith('.json'):\n",
    "        continue\n",
    "\n",
    "    if task not in fname:\n",
    "        continue\n",
    "    with open(fname, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        results[os.path.splitext(fname)[0].replace(eval_results_path + '/', '').replace(\n",
    "            'unieval_', '').replace(f'-DecoderDisc-{task}', '')] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfma = results.pop(f'{task}-MFMA', None)\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "if mfma is not None:\n",
    "    mfma = {os.path.basename(k): v for k, v in mfma.items()}\n",
    "    mfma_df = pd.DataFrame(mfma).T\n",
    "    df = df.merge(mfma_df, left_index=True, right_index=True)\n",
    "\n",
    "if task == 'wow':\n",
    "    df['prob.'] = [-1] * len(df)  # get these later!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KF1</th>\n",
       "      <th>K-Copy</th>\n",
       "      <th>F1</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>RougeL</th>\n",
       "      <th>chrF</th>\n",
       "      <th>meteor</th>\n",
       "      <th>naturalness</th>\n",
       "      <th>coherence</th>\n",
       "      <th>groundedness</th>\n",
       "      <th>understandability</th>\n",
       "      <th>prob.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline-wow-openai_gpt-3.5-turbo</th>\n",
       "      <td>49.41</td>\n",
       "      <td>39.71</td>\n",
       "      <td>30.32</td>\n",
       "      <td>6.91</td>\n",
       "      <td>26.24</td>\n",
       "      <td>34.95</td>\n",
       "      <td>31.67</td>\n",
       "      <td>57.62</td>\n",
       "      <td>96.41</td>\n",
       "      <td>96.15</td>\n",
       "      <td>57.27</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline-wow-openai_text-davinci-003</th>\n",
       "      <td>25.91</td>\n",
       "      <td>28.22</td>\n",
       "      <td>22.32</td>\n",
       "      <td>3.01</td>\n",
       "      <td>18.70</td>\n",
       "      <td>27.86</td>\n",
       "      <td>23.06</td>\n",
       "      <td>42.77</td>\n",
       "      <td>98.07</td>\n",
       "      <td>92.42</td>\n",
       "      <td>42.51</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline-wow-google-flan-t5-xl</th>\n",
       "      <td>34.50</td>\n",
       "      <td>37.07</td>\n",
       "      <td>21.18</td>\n",
       "      <td>6.81</td>\n",
       "      <td>19.64</td>\n",
       "      <td>24.88</td>\n",
       "      <td>18.53</td>\n",
       "      <td>71.69</td>\n",
       "      <td>82.21</td>\n",
       "      <td>75.70</td>\n",
       "      <td>70.90</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline-wow-google-flan-t5-xxl</th>\n",
       "      <td>28.20</td>\n",
       "      <td>32.33</td>\n",
       "      <td>19.11</td>\n",
       "      <td>5.53</td>\n",
       "      <td>17.55</td>\n",
       "      <td>24.15</td>\n",
       "      <td>17.16</td>\n",
       "      <td>72.37</td>\n",
       "      <td>84.24</td>\n",
       "      <td>75.51</td>\n",
       "      <td>71.68</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline-wow-bigscience-T0pp</th>\n",
       "      <td>26.94</td>\n",
       "      <td>28.80</td>\n",
       "      <td>17.57</td>\n",
       "      <td>4.13</td>\n",
       "      <td>16.14</td>\n",
       "      <td>19.84</td>\n",
       "      <td>13.37</td>\n",
       "      <td>52.79</td>\n",
       "      <td>85.26</td>\n",
       "      <td>70.14</td>\n",
       "      <td>52.27</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline-wow-google-flan-t5-xl-sft</th>\n",
       "      <td>39.85</td>\n",
       "      <td>37.79</td>\n",
       "      <td>28.08</td>\n",
       "      <td>9.41</td>\n",
       "      <td>25.11</td>\n",
       "      <td>31.17</td>\n",
       "      <td>25.40</td>\n",
       "      <td>76.44</td>\n",
       "      <td>92.36</td>\n",
       "      <td>95.16</td>\n",
       "      <td>75.83</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fudge-wow-google-flan-t5-xl-fudge-RAND</th>\n",
       "      <td>55.30</td>\n",
       "      <td>54.04</td>\n",
       "      <td>29.43</td>\n",
       "      <td>11.72</td>\n",
       "      <td>27.35</td>\n",
       "      <td>31.50</td>\n",
       "      <td>26.00</td>\n",
       "      <td>73.68</td>\n",
       "      <td>88.20</td>\n",
       "      <td>83.53</td>\n",
       "      <td>72.90</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25</th>\n",
       "      <td>50.20</td>\n",
       "      <td>50.10</td>\n",
       "      <td>27.86</td>\n",
       "      <td>10.57</td>\n",
       "      <td>26.01</td>\n",
       "      <td>29.84</td>\n",
       "      <td>24.51</td>\n",
       "      <td>74.14</td>\n",
       "      <td>88.35</td>\n",
       "      <td>81.10</td>\n",
       "      <td>73.34</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_mcts-wow-google-flan-t5-xl-RAND</th>\n",
       "      <td>55.54</td>\n",
       "      <td>54.21</td>\n",
       "      <td>29.56</td>\n",
       "      <td>11.69</td>\n",
       "      <td>27.48</td>\n",
       "      <td>31.60</td>\n",
       "      <td>26.08</td>\n",
       "      <td>74.54</td>\n",
       "      <td>88.16</td>\n",
       "      <td>83.90</td>\n",
       "      <td>73.76</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fudge-wow-google-flan-t5-xl-fudge-RIPA</th>\n",
       "      <td>58.19</td>\n",
       "      <td>56.56</td>\n",
       "      <td>30.71</td>\n",
       "      <td>12.74</td>\n",
       "      <td>28.27</td>\n",
       "      <td>33.40</td>\n",
       "      <td>28.10</td>\n",
       "      <td>68.44</td>\n",
       "      <td>90.51</td>\n",
       "      <td>87.86</td>\n",
       "      <td>67.84</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_mcts-wow-google-flan-t5-xl-RIPA</th>\n",
       "      <td>56.06</td>\n",
       "      <td>51.90</td>\n",
       "      <td>30.54</td>\n",
       "      <td>11.42</td>\n",
       "      <td>27.43</td>\n",
       "      <td>35.22</td>\n",
       "      <td>28.92</td>\n",
       "      <td>59.24</td>\n",
       "      <td>92.77</td>\n",
       "      <td>91.78</td>\n",
       "      <td>58.77</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    KF1  K-Copy     F1   BLEU  \\\n",
       "baseline-wow-openai_gpt-3.5-turbo                 49.41   39.71  30.32   6.91   \n",
       "baseline-wow-openai_text-davinci-003              25.91   28.22  22.32   3.01   \n",
       "baseline-wow-google-flan-t5-xl                    34.50   37.07  21.18   6.81   \n",
       "baseline-wow-google-flan-t5-xxl                   28.20   32.33  19.11   5.53   \n",
       "baseline-wow-bigscience-T0pp                      26.94   28.80  17.57   4.13   \n",
       "baseline-wow-google-flan-t5-xl-sft                39.85   37.79  28.08   9.41   \n",
       "fudge-wow-google-flan-t5-xl-fudge-RAND            55.30   54.04  29.43  11.72   \n",
       "nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25  50.20   50.10  27.86  10.57   \n",
       "ppl_mcts-wow-google-flan-t5-xl-RAND               55.54   54.21  29.56  11.69   \n",
       "fudge-wow-google-flan-t5-xl-fudge-RIPA            58.19   56.56  30.71  12.74   \n",
       "ppl_mcts-wow-google-flan-t5-xl-RIPA               56.06   51.90  30.54  11.42   \n",
       "\n",
       "                                                  RougeL   chrF  meteor  \\\n",
       "baseline-wow-openai_gpt-3.5-turbo                  26.24  34.95   31.67   \n",
       "baseline-wow-openai_text-davinci-003               18.70  27.86   23.06   \n",
       "baseline-wow-google-flan-t5-xl                     19.64  24.88   18.53   \n",
       "baseline-wow-google-flan-t5-xxl                    17.55  24.15   17.16   \n",
       "baseline-wow-bigscience-T0pp                       16.14  19.84   13.37   \n",
       "baseline-wow-google-flan-t5-xl-sft                 25.11  31.17   25.40   \n",
       "fudge-wow-google-flan-t5-xl-fudge-RAND             27.35  31.50   26.00   \n",
       "nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25   26.01  29.84   24.51   \n",
       "ppl_mcts-wow-google-flan-t5-xl-RAND                27.48  31.60   26.08   \n",
       "fudge-wow-google-flan-t5-xl-fudge-RIPA             28.27  33.40   28.10   \n",
       "ppl_mcts-wow-google-flan-t5-xl-RIPA                27.43  35.22   28.92   \n",
       "\n",
       "                                                  naturalness  coherence  \\\n",
       "baseline-wow-openai_gpt-3.5-turbo                       57.62      96.41   \n",
       "baseline-wow-openai_text-davinci-003                    42.77      98.07   \n",
       "baseline-wow-google-flan-t5-xl                          71.69      82.21   \n",
       "baseline-wow-google-flan-t5-xxl                         72.37      84.24   \n",
       "baseline-wow-bigscience-T0pp                            52.79      85.26   \n",
       "baseline-wow-google-flan-t5-xl-sft                      76.44      92.36   \n",
       "fudge-wow-google-flan-t5-xl-fudge-RAND                  73.68      88.20   \n",
       "nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25        74.14      88.35   \n",
       "ppl_mcts-wow-google-flan-t5-xl-RAND                     74.54      88.16   \n",
       "fudge-wow-google-flan-t5-xl-fudge-RIPA                  68.44      90.51   \n",
       "ppl_mcts-wow-google-flan-t5-xl-RIPA                     59.24      92.77   \n",
       "\n",
       "                                                  groundedness  \\\n",
       "baseline-wow-openai_gpt-3.5-turbo                        96.15   \n",
       "baseline-wow-openai_text-davinci-003                     92.42   \n",
       "baseline-wow-google-flan-t5-xl                           75.70   \n",
       "baseline-wow-google-flan-t5-xxl                          75.51   \n",
       "baseline-wow-bigscience-T0pp                             70.14   \n",
       "baseline-wow-google-flan-t5-xl-sft                       95.16   \n",
       "fudge-wow-google-flan-t5-xl-fudge-RAND                   83.53   \n",
       "nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25         81.10   \n",
       "ppl_mcts-wow-google-flan-t5-xl-RAND                      83.90   \n",
       "fudge-wow-google-flan-t5-xl-fudge-RIPA                   87.86   \n",
       "ppl_mcts-wow-google-flan-t5-xl-RIPA                      91.78   \n",
       "\n",
       "                                                  understandability  prob.  \n",
       "baseline-wow-openai_gpt-3.5-turbo                             57.27   -100  \n",
       "baseline-wow-openai_text-davinci-003                          42.51   -100  \n",
       "baseline-wow-google-flan-t5-xl                                70.90   -100  \n",
       "baseline-wow-google-flan-t5-xxl                               71.68   -100  \n",
       "baseline-wow-bigscience-T0pp                                  52.27   -100  \n",
       "baseline-wow-google-flan-t5-xl-sft                            75.83   -100  \n",
       "fudge-wow-google-flan-t5-xl-fudge-RAND                        72.90   -100  \n",
       "nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25              73.34   -100  \n",
       "ppl_mcts-wow-google-flan-t5-xl-RAND                           73.76   -100  \n",
       "fudge-wow-google-flan-t5-xl-fudge-RIPA                        67.84   -100  \n",
       "ppl_mcts-wow-google-flan-t5-xl-RIPA                           58.77   -100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markdown\n",
      "|                                                  |   KF1 |   K-Copy |    F1 |   BLEU |   RougeL |   chrF |   meteor |   naturalness |   coherence |   groundedness |   understandability |   prob. |\n",
      "|:-------------------------------------------------|------:|---------:|------:|-------:|---------:|-------:|---------:|--------------:|------------:|---------------:|--------------------:|--------:|\n",
      "| baseline-wow-openai_gpt-3.5-turbo                | 49.41 |    39.71 | 30.32 |   6.91 |    26.24 |  34.95 |    31.67 |         57.62 |       96.41 |          96.15 |               57.27 |    -100 |\n",
      "| baseline-wow-openai_text-davinci-003             | 25.91 |    28.22 | 22.32 |   3.01 |    18.7  |  27.86 |    23.06 |         42.77 |       98.07 |          92.42 |               42.51 |    -100 |\n",
      "| baseline-wow-google-flan-t5-xl                   | 34.5  |    37.07 | 21.18 |   6.81 |    19.64 |  24.88 |    18.53 |         71.69 |       82.21 |          75.7  |               70.9  |    -100 |\n",
      "| baseline-wow-google-flan-t5-xxl                  | 28.2  |    32.33 | 19.11 |   5.53 |    17.55 |  24.15 |    17.16 |         72.37 |       84.24 |          75.51 |               71.68 |    -100 |\n",
      "| baseline-wow-bigscience-T0pp                     | 26.94 |    28.8  | 17.57 |   4.13 |    16.14 |  19.84 |    13.37 |         52.79 |       85.26 |          70.14 |               52.27 |    -100 |\n",
      "| baseline-wow-google-flan-t5-xl-sft               | 39.85 |    37.79 | 28.08 |   9.41 |    25.11 |  31.17 |    25.4  |         76.44 |       92.36 |          95.16 |               75.83 |    -100 |\n",
      "| fudge-wow-google-flan-t5-xl-fudge-RAND           | 55.3  |    54.04 | 29.43 |  11.72 |    27.35 |  31.5  |    26    |         73.68 |       88.2  |          83.53 |               72.9  |    -100 |\n",
      "| nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25 | 50.2  |    50.1  | 27.86 |  10.57 |    26.01 |  29.84 |    24.51 |         74.14 |       88.35 |          81.1  |               73.34 |    -100 |\n",
      "| ppl_mcts-wow-google-flan-t5-xl-RAND              | 55.54 |    54.21 | 29.56 |  11.69 |    27.48 |  31.6  |    26.08 |         74.54 |       88.16 |          83.9  |               73.76 |    -100 |\n",
      "| fudge-wow-google-flan-t5-xl-fudge-RIPA           | 58.19 |    56.56 | 30.71 |  12.74 |    28.27 |  33.4  |    28.1  |         68.44 |       90.51 |          87.86 |               67.84 |    -100 |\n",
      "| ppl_mcts-wow-google-flan-t5-xl-RIPA              | 56.06 |    51.9  | 30.54 |  11.42 |    27.43 |  35.22 |    28.92 |         59.24 |       92.77 |          91.78 |               58.77 |    -100 |\n",
      "\n",
      "latex\n",
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} &    KF1 &  K-Copy &     F1 &   BLEU &  RougeL &   chrF &  meteor &  naturalness &  coherence &  groundedness &  understandability &  prob. \\\\\n",
      "\\midrule\n",
      "baseline-wow-openai\\_gpt-3.5-turbo                &  49.41 &   39.71 &  30.32 &   6.91 &   26.24 &  34.95 &   31.67 &        57.62 &      96.41 &         96.15 &              57.27 &   -100 \\\\\n",
      "baseline-wow-openai\\_text-davinci-003             &  25.91 &   28.22 &  22.32 &   3.01 &   18.70 &  27.86 &   23.06 &        42.77 &      98.07 &         92.42 &              42.51 &   -100 \\\\\n",
      "baseline-wow-google-flan-t5-xl                   &  34.50 &   37.07 &  21.18 &   6.81 &   19.64 &  24.88 &   18.53 &        71.69 &      82.21 &         75.70 &              70.90 &   -100 \\\\\n",
      "baseline-wow-google-flan-t5-xxl                  &  28.20 &   32.33 &  19.11 &   5.53 &   17.55 &  24.15 &   17.16 &        72.37 &      84.24 &         75.51 &              71.68 &   -100 \\\\\n",
      "baseline-wow-bigscience-T0pp                     &  26.94 &   28.80 &  17.57 &   4.13 &   16.14 &  19.84 &   13.37 &        52.79 &      85.26 &         70.14 &              52.27 &   -100 \\\\\n",
      "baseline-wow-google-flan-t5-xl-sft               &  39.85 &   37.79 &  28.08 &   9.41 &   25.11 &  31.17 &   25.40 &        76.44 &      92.36 &         95.16 &              75.83 &   -100 \\\\\n",
      "fudge-wow-google-flan-t5-xl-fudge-RAND           &  55.30 &   54.04 &  29.43 &  11.72 &   27.35 &  31.50 &   26.00 &        73.68 &      88.20 &         83.53 &              72.90 &   -100 \\\\\n",
      "nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25 &  50.20 &   50.10 &  27.86 &  10.57 &   26.01 &  29.84 &   24.51 &        74.14 &      88.35 &         81.10 &              73.34 &   -100 \\\\\n",
      "ppl\\_mcts-wow-google-flan-t5-xl-RAND              &  55.54 &   54.21 &  29.56 &  11.69 &   27.48 &  31.60 &   26.08 &        74.54 &      88.16 &         83.90 &              73.76 &   -100 \\\\\n",
      "fudge-wow-google-flan-t5-xl-fudge-RIPA           &  58.19 &   56.56 &  30.71 &  12.74 &   28.27 &  33.40 &   28.10 &        68.44 &      90.51 &         87.86 &              67.84 &   -100 \\\\\n",
      "ppl\\_mcts-wow-google-flan-t5-xl-RIPA              &  56.06 &   51.90 &  30.54 &  11.42 &   27.43 &  35.22 &   28.92 &        59.24 &      92.77 &         91.78 &              58.77 &   -100 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45523/474750440.py:40: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(out_df.to_latex(na_rep=''))\n"
     ]
    }
   ],
   "source": [
    "# pip install tabulate\n",
    "\n",
    "if task == 'wow':\n",
    "    # TODO: column names for unieval dialogue\n",
    "    columns = ['KF1', 'K-Copy', 'F1', 'BLEU', 'RougeL',  'chrF', 'meteor', \"naturalness\", \"coherence\", \"groundedness\", \"understandability\", 'prob.']\n",
    "    # for order\n",
    "    rows = ['baseline-wow-openai_gpt-3.5-turbo',  # zeroshot\n",
    "            'baseline-wow-openai_text-davinci-003',\n",
    "            'baseline-wow-google-flan-t5-xl',\n",
    "            'baseline-wow-google-flan-t5-xxl',\n",
    "            'baseline-wow-bigscience-T0pp',\n",
    "            'baseline-wow-google-flan-t5-xl-sft',  # sft\n",
    "            'fudge-wow-google-flan-t5-xl-fudge-RAND',  # guided decoding baseline\n",
    "            'nado-wow-google-flan-t5-xl-nado-ALL-v2-alpha0.25',\n",
    "            'ppl_mcts-wow-google-flan-t5-xl-RAND',\n",
    "            'fudge-wow-google-flan-t5-xl-fudge-RIPA',  # KWD\n",
    "            'ppl_mcts-wow-google-flan-t5-xl-RIPA',  # KCTS\n",
    "            ]\n",
    "else:\n",
    "    columns = ['KF1', 'K-Copy', 'F1', 'BLEU', 'RougeL', 'chrF', 'meteor', 'coherence', 'consistency', 'fluency',  'relevance']\n",
    "    rows = ['baseline-cnn_dailymail-openai_gpt-3.5-turbo',  # zeroshot\n",
    "            'baseline-cnn_dailymail-openai_text-davinci-003',\n",
    "            'baseline-cnn_dailymail-google-flan-t5-xxl',\n",
    "            'baseline-cnn_dailymail-bigscience-T0pp',\n",
    "            'baseline-cnn_dailymail-google-flan-t5-xl',\n",
    "            'fudge-cnn_dailymail-google-flan-t5-xl-fudge-RAND',  # guided decoding baseline\n",
    "            'nado-cnn_dailymail-google-flan-t5-xl-nado-ALL-v2-alpha0.25',\n",
    "            'ppl_mcts-cnn_dailymail-google-flan-t5-xl-RAND',\n",
    "            'fudge-cnn_dailymail-google-flan-t5-xl-fudge-RIPA',  # KWD\n",
    "            \"ppl_mcts-cnn_dailymail-google-flan-t5-xl-RIPA\",  # KCTS\n",
    "            ]\n",
    "\n",
    "out_df = (df.loc[rows, columns] * 100).round(2)\n",
    "out_df['chrF'] = (out_df['chrF'] / 100).round(2)\n",
    "display(out_df)\n",
    "print('markdown')\n",
    "print(out_df.to_markdown())\n",
    "print()\n",
    "print('latex')\n",
    "print(out_df.to_latex(na_rep=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
